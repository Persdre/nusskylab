Focus group meeting response: adviser
==============================================================

### General feedback: discussion during the meeting
* Checking who had already evaluated / or sent feedback on an evaluation: we should have status columns(similar to "dropped" status) for submission status, peer evaluation status so that with just one glance adviser can figure out who have not submitted and remind them
* Change tab order for adviser homepage: move the current first tab to last as advisers rarely use them
* In the evaluations, even the buttons of the teams should have indicators for dropped status
* Forms too verbose Three radio fields take up more than one page. Suggested way of presenting options: Poor 1 [?] (Tooltip for explanation for this option)   2 [?]   3 [?]   4 [?]   [?] 5 Best 
* [Pushed to Future]Hosting of videos
* Auto expand text boxes when close to full
* [Questionable]Separate bonus point away from the evaluation.
* Dropdown for team selection not clear about whether the new team is being evaluation
* [Deprecated]Team's proposed LoA could be separated out so that it doesn't bias review?
* Autosave text and input / prompt for leaving page
* [Deprecated]Comment function for README / Log
* Summary composite page for the EG with respect to each evaluation question for Likert scale and textboxes / Quick Fix: use anchors to jump to a particular form on a concatenated page.
* "More info" tab in adviser homepage is misleading
* [For Instructor Prof Min]Ask for advisers' opinion on peer evaluation questions: Is a question clear, ambiguous, not necessary? Should we add more?
* Enable users to view all past submissions, all past peer evaluations(by providing link to viewing them in some place) when you are editing/submitting a peer evaluation.
* [Deprecated]Structure the project log like one record by one record and do appending for next milestones.
* Evaluating relationship: explore use of D3(or similar stuff) to represent all relationships as graph



### Suggestions from responses of Pre-meeting Questionnaire
**What do you remember about Skylab? How was the experience with use of Skylab during the summer?**
- [U1] Too many clicks to various functionality.
- [U2] Skylab: Effective tool to help monitor each team's evaluations. Very helpful.
  Experience: Despite the few issues (especially at the very beginning like Milestone 1), majority usage was actually smooth. Didn't encounter too many technical issues utilizing it.

**Any problems raised in Eval 1 from you or your students? What were those?**
- [U1] *No response*
- [U2] Eval 1 -- generally was just the late opening of Milestone 1 submission on Skylab as the feature was still being developed. Aside from that there weren't actually any complaints on this matter.
Many students, at the point of doing Orbital, are not familiar with the concept of User Stories. As such, they have a tendency to not write User Stories, leading them to be negatively marked during evaluations.
Maybe, when creating submission guidelines for Milestone 1, a link to how to write user stories can also be given. Not too sure if this was done for this year's Orbital.

**Any problems raised in Eval 2 from you or your students? What were those?**
- [U1] *No response*
- [U2] Eval 2 was more related to deadlines, when students were rushing to finish work and kept requesting for extensions. Not a problem per say but just something I wanted to point out.

**Any problems raised in Eval 3 from you or your students? What were those?**
- [U1] *No response*
- [U2] Once more, it's related to the request for additional deadline extensions. The extensions this time around would result in the submission deadline overlapping into the start of the semester, so it was difficult to grant them without keeping in mind that students would be returning to college soon and won't really have time to continue working on it.
### Suggestions from responses of During&Post-meeting Questionnaire

**Any problems raised in Feedback from you or your students? What were those?**
- [U1] *No response*
- [U2] Feedback form wise:
  Testing section for feedback form in Milestone 2/3 is a little ambiguous to most students -- most students tend to fill this segment in without fully understanding what each testing method means, leading to a discrepancy across each peer group's evaluations. Since most students also don't actually state the testing methods they employed in their readme, it is difficult to tell if they've really done any additional testing.
  If possible, maybe make the entire box for the bullet point clickable instead of just the bullet point itself? More user friendly that way :)
  "Bonus point!" makes it difficult to tell exactly what score peer teams are giving to the group they are evaluating. Sometimes, teams may feel that the team has done good enough for Vostok, but not enough for Gemini. Yet at the same time, they feel that the work the team has put in is deserving of a bonus point. When teams select "bonus point" option, it is hard to tell if they mean "Apollo 11 work + 1 bonus point" or another achievement level with bonus point altogether. May want to separate the "bonus point" segment from the evaluation

**Overall, what do you think of Skylab's usefulness and usability during the summer?**
- [U1] Really helpful
- [U2] Really helpful

**Overall, what do you think of Skylab's user-friendliness during the summer?**
- [U1] Can but hard to figure out how to do things
- [U2] Fairly easy to figure out how to do things

**Any comments?**
- [U1] See During&Post Meeting.
- [U2] For Splashdown this Orbital, advisors did not get to vote. It was kind of assumed that we'd automatically be given voting IDs since we were involved in Orbital directly during the summer. As such, there was some confusion during Splashdown among advisors whether we had a voting ID to help vote as well.
  Also, this time around, there were no registration counters set up outside SR1 to carry out last-minute registrations compared to Orbital 2014. Not sure if this was intended, but the counter was a good way to give last-minute attendees a chance to vote, and at the same time, for Orbital teams to stash their posters there before they arrive.
  The switch around between session 1 and 2 was a lot smoother this time, and since talks were kept to post-session 2, it ensured that speakers would be listened to instead of having their voices drowned out in the chaos. Good to retain this format for future Orbital!
  Slack proved to be an excellent tool for sharing announcements and information to students. Would be great if Slack, or a similar tool, is used in future Orbital runs.
  The tricky thing regarding Mission Control is that the later it is in the summer, the less likely students are keen to participate, because of how they probably cannot apply the knowledge to their own projects although it is an opportunity to learn something new. As such, I suggest having something like "Mission Control Days" similar to Liftoff -- optional attendance, with each workshop being held in 2 hr sprints across the entire day. Students can attend the ones they are interested in. This would probably be best held sometime between before Milestone 1 submission to before Milestone 2 submission so that students can try to apply what they've learned to their project. Unsure if this would be a good idea since it is condensing a lot of things into a single day, which may overwork students.
  Seems like students are not very keen in utilizing Python/GAE as their project, even if they're doing a website. Maybe a different development platform could be taught this time around? Not sure what platforms would be a good idea though.

<br>
### Suggestions from responses of During&Post-meeting Questionnaire
**How is the login screen and functionality? Are they working as expected?**
- [U1] NUS login works fine.
- [U2] Login screen and functionality work fine, albeit looking a bit plain (just me being picky in design). Not major issues to note!

**How does the home screen look like? Anything misplaced? Or anything missing?**
- [U1] Use "Submit evaluations" as default tab. "View all your teams" is not really necessary.
- [U2] Home screen is okay pre-login. Post-login, suggest that "View all your teams" be moved to the last -- advisors don't really look at team details usually.
 Instead, "Submit Evaluations" tab should come first. Under "Submit Evaluations" the 'Status' column is a little bit unclear -- status of what? I understand that it's related to their README/Log submissions, so maybe renaming the column to something more obvious would be better. Or just change 'View' to 'View README/Log' instead.

**Anything working against how you complete your tasks? How can the system be modified to better suit your needs?**
- [U1] Viewing if evaluations were done should be done in a lot less clicks.
- [U2] To see each team's progress in terms of what peer teams they have evaluated, the current process is:
  1. Click on a team name
  2. Click on a member name
  3. Click on "Evaluate Other teams" tab
  4. View which groups this member/team has evaluated.
  This process could probably be better streamlined. Advisors would want to know this information quite often, especially when close to peer evaluation deadlines, so maybe this information could be moved to the main page for advisors (a 4th tab aside from "View all your teams", "Submit evaluations" and "View more info" perhaps?)

**When viewing students' milestone submission, what feels unnatural or missing? Would you like quick comparison of previous milestone submission of this team for example?**
- [U1] Yes, previous submissions will be useful, so as to compare if they have managed to add in things mentioned in the previous evaluation.
- [U2] Would be nice if embedding of videos could be done on Skylab (so students can just watch the videos on Skylab without redirection), but understandable that different video hosting sides have different embedding rules so this may be tough to implement.
  Aside from that, viewing of students submission has no major issue.

**How is the process of submitting peer evaluation? Anything to improve?**
- [U1] Suggest all options to use "Strongly Disagree / ... / Strongly Agree" to keep it consistent.
- [U2] Evaluating teams currently involves the following process:
  1. Have a tab opened for team's README/Log, and a tab opened for team's evaluation form.
  2. Tabs are placed side by side (so each tab takes 50% of screen, one on left, one on right) for better visibility of README while evaluating.
  Not sure how to improve on this process further, because this has been the most convenient way I've done all my evaluations.
  Unsure of purpose of dropdown menu on top that states which team we're evaluating currently. I see that the dropdown is now disabled, but maybe instead of a dropdown menu (since we can't interact with it), just change it to a regular un-interactable text/input box with the team info? :)

**How is viewing of feedback from your students? Anything to suggest for improvement?**
- [U1] *No response*
- [U2] No issues here, very neatly organized currently.

**Any features/extensions you would like to see? Any suggestions?**
- [U1] Fixed project log format for all students instead of free-style.
- [U2] The following suggestions lean more towards the students side.
  "View all submitted evaluations" function is very nice currently, but could be streamlined further. Currently, each team's evaluation form is shown separately, so a lot of scrolling is needed.
  Perhaps, a consolidated form containing what each team has selected could be used instead. For example, "Team A" and "Team B" selected Agree and "Team C" selected "Neutral", so in the consolidated form, under Agree, we see "Team A" and "Team B" and under Neutral, we see "Team C". This way, teams can see in one shot what each evaluator has graded them. This could also be extended to the text feedback, so that all the feedback is available in one single evaluation form instead of split into one form per group.

**Have you used/seen similar systems before? Anything can Skylab borrow from them?**
- [U1] *No response*
- [U2] Not really. Unable to provide feedback for this.

**What are other things you want to tell us?**
- [U1] Thanks!
- [U2] Nothing else! Everything I wanted to say is available in each section.
